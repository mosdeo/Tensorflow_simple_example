{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mp_image\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from functools import partial\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_filename(full_name, filename_extension):\n",
    "    return full_name.split(sep=\".\"+filename_extension)[0].split(sep='/')[-1]\n",
    "\n",
    "extract_png_filename = partial(extract_filename, filename_extension='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取訓練資料到陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ground truth\n",
    "df_UNBC_pain_score = pd.read_csv('./UNBC_pain_score_table.csv') # 讀取疼痛分數資料\n",
    "df_UNBC_pain_score.sort_values(by=['PSPI'], ascending=False, inplace=True) # 高分排前面\n",
    "df_UNBC_pain_score.reset_index(drop=True, inplace=True) # 更新 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 46376\n"
     ]
    }
   ],
   "source": [
    "# feed training data\n",
    "\n",
    "# 建構檔案路徑\n",
    "# croped_face_folder = '../datalake/crop_face_from_UNBC_pain/'\n",
    "file_ext = '.png'\n",
    "# 讀取目錄底下所有檔案絕對路徑\n",
    "paths_croped_face_img = subprocess.getoutput('find /home/lky/crop_face_from_UNBC_pain/ -type f -name \"*.png\"').split('\\n')\n",
    "print(\"count = {}\".format(+len(paths_croped_face_img)))\n",
    "# paths_croped_face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 只使用有被偵測到臉的\n",
    "df_croped_face_img = pd.DataFrame(\n",
    "                        data = {'full_name':paths_croped_face_img, \n",
    "                                'file_name':[i for i in map(extract_png_filename, paths_croped_face_img)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46376\n"
     ]
    }
   ],
   "source": [
    "# 合併檔案路徑表和分數表，之後只用這張表，清空不必要變數\n",
    "df_croped_face_img = df_croped_face_img.merge(right=df_UNBC_pain_score, on='file_name', how='inner')\n",
    "paths_croped_face_img = []\n",
    "df_UNBC_pain_score = []\n",
    "print(df_croped_face_img.index.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把臉部 ROI 轉化成可以直接輸入 nn 的 vector\n",
    "def rect_img_to_input_vector(raw_img):\n",
    "    import skimage\n",
    "    gray_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2GRAY) #轉灰\n",
    "#     gray_img = cv2.equalizeHist(gray_img) #亮度正規化\n",
    "    norm_size_img = cv2.resize(gray_img,(128, 128)) #統一尺寸\n",
    "    max_pooling_img = skimage.measure.block_reduce(norm_size_img, (4,4), np.max) #最大池化\n",
    "    reshape_img = np.reshape(max_pooling_img, [-1]) #拉平\n",
    "    return reshape_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 讀取影像函式\n",
    "def read_dataset(list_data_path):\n",
    "    import skimage.measure\n",
    "    dataset = []\n",
    "    read_fail_set = []\n",
    "    print(\"read img {:5d}, {}\".format(len(dataset), subprocess.getoutput('date')))\n",
    "    #-------------------------------------------------------------------\n",
    "    for img_full_name in list_data_path:\n",
    "        try:\n",
    "            raw_img = cv2.imread(img_full_name)\n",
    "        except:\n",
    "            read_fail_set.append(img_full_name)\n",
    "            print(img_full_name, len(read_fail_set))\n",
    "\n",
    "        dataset.append(rect_img_to_input_vector(raw_img)) #push in list\n",
    "\n",
    "        if(0 == len(dataset)%5000):\n",
    "            print(\"read img {:5d}, {}\".format(len(dataset), subprocess.getoutput('date')))\n",
    "    #-------------------------------------------------------------------\n",
    "    print(\"read img {:5d}, {}\".format(len(dataset), subprocess.getoutput('date')))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類用資料編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 產生 one-hot labels\n",
    "PSPI_one_hot = tf.Session().run(\n",
    "                    tf.one_hot(indices=df_croped_face_img['PSPI'].values, # 被 one-hot data\n",
    "                    depth = len(df_croped_face_img['PSPI'].unique()), # 取集合大小\n",
    "                    dtype = tf.int32))\n",
    "df_croped_face_img['PSPI_one_hot'] = pd.Series(PSPI_one_hot.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_croped_face_img['PSPI_one_hot'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割預備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle\n",
    "df_croped_face_img = df_croped_face_img.reindex(index = np.random.permutation(df_croped_face_img.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 只以 PSPI 排序\n",
    "df_croped_face_img.sort_values(by=['PSPI'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 重新給 index，不留原本的 index\n",
    "df_croped_face_img.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割 [ 訓練集 , 測試集 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 雙數抽樣(訓練集)\n",
    "df_croped_face_img_training = df_croped_face_img.iloc[[i for i in df_croped_face_img.index if i&1==0 ],:]\n",
    "\n",
    "# 單數抽樣(測試集)\n",
    "df_croped_face_img_validation = df_croped_face_img.iloc[[i for i in df_croped_face_img.index if i&1==1 ],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料集影像內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read img     0, 一  9月 11 13:50:04 CST 2017\n",
      "read img  5000, 一  9月 11 13:50:17 CST 2017\n",
      "read img 10000, 一  9月 11 13:50:29 CST 2017\n",
      "read img 15000, 一  9月 11 13:50:43 CST 2017\n",
      "read img 20000, 一  9月 11 13:50:55 CST 2017\n",
      "read img 23188, 一  9月 11 13:51:02 CST 2017\n",
      "read img     0, 一  9月 11 13:51:02 CST 2017\n",
      "read img  5000, 一  9月 11 13:51:13 CST 2017\n",
      "read img 10000, 一  9月 11 13:51:23 CST 2017\n",
      "read img 15000, 一  9月 11 13:51:35 CST 2017\n",
      "read img 20000, 一  9月 11 13:51:47 CST 2017\n",
      "read img 23188, 一  9月 11 13:51:55 CST 2017\n"
     ]
    }
   ],
   "source": [
    "training_img_raw_dataset = read_dataset(df_croped_face_img_training['full_name'])\n",
    "validation_img_raw_dataset = read_dataset(df_croped_face_img_validation['full_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一  9月 11 13:51:57 CST 2017\n"
     ]
    }
   ],
   "source": [
    "# 對資料集正規化\n",
    "## 正規化的單位並不是一張影像，而是一個位置，所以是對每一張影像的同一個位置的像素正規化\n",
    "from sklearn import preprocessing\n",
    "training_img_dataset = preprocessing.minmax_scale(training_img_raw_dataset,(-0.9, 0.9))\n",
    "validation_img_dataset = preprocessing.minmax_scale(validation_img_raw_dataset,(-0.9, 0.9))\n",
    "print(\"{}\".format(subprocess.getoutput('date')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建構神經網路\n",
    "\n",
    "# 單層建構函式\n",
    "def make_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf. random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "in_size = 32*32\n",
    "out_size = len(df_croped_face_img['PSPI'].unique())\n",
    "\n",
    "# 定義輸出入變數\n",
    "xs = tf.placeholder(tf.float32, [None, in_size])\n",
    "ys = tf.placeholder(tf.float32, [None, out_size])\n",
    "\n",
    "# 定義多層流程\n",
    "layer_hidden_1 = make_layer(xs, in_size, 32, activation_function = tf.nn.tanh)\n",
    "layer_output   = make_layer(layer_hidden_1, 32, out_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定義損失函數\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=layer_output, labels=ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定義學習方法\n",
    "learning_rate = 0.005\n",
    "momentum = 0.01\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate, momentum).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 變數初始化\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = None, cost = 12.643253326416016, 一  9月 11 13:51:58 CST 2017\n",
      "epoch = 0, cost = 0.6975019574165344, 一  9月 11 13:52:31 CST 2017\n",
      "epoch = 1, cost = 0.6209016442298889, 一  9月 11 13:52:59 CST 2017\n",
      "epoch = 2, cost = 0.5433548092842102, 一  9月 11 13:53:35 CST 2017\n",
      "epoch = 3, cost = 0.5000848174095154, 一  9月 11 13:54:05 CST 2017\n",
      "epoch = 4, cost = 0.4680270552635193, 一  9月 11 13:54:38 CST 2017\n",
      "epoch = 5, cost = 0.4456261694431305, 一  9月 11 13:55:16 CST 2017\n",
      "epoch = 6, cost = 0.43079474568367004, 一  9月 11 13:55:48 CST 2017\n",
      "epoch = 7, cost = 0.4158190190792084, 一  9月 11 13:56:10 CST 2017\n",
      "epoch = 8, cost = 0.39071857929229736, 一  9月 11 13:56:29 CST 2017\n",
      "epoch = 9, cost = 0.3802110254764557, 一  9月 11 13:56:46 CST 2017\n",
      "epoch = 10, cost = 0.3723045885562897, 一  9月 11 13:57:05 CST 2017\n",
      "epoch = 11, cost = 0.3664077818393707, 一  9月 11 13:57:23 CST 2017\n",
      "epoch = 12, cost = 0.34567633271217346, 一  9月 11 13:57:47 CST 2017\n",
      "epoch = 13, cost = 0.34586986899375916, 一  9月 11 13:58:06 CST 2017\n",
      "epoch = 14, cost = 0.3309105634689331, 一  9月 11 13:58:24 CST 2017\n",
      "epoch = 15, cost = 0.32465919852256775, 一  9月 11 13:58:44 CST 2017\n",
      "epoch = 16, cost = 0.32124313712120056, 一  9月 11 13:59:03 CST 2017\n",
      "epoch = 17, cost = 0.32211342453956604, 一  9月 11 13:59:23 CST 2017\n",
      "epoch = 18, cost = 0.316243439912796, 一  9月 11 14:00:00 CST 2017\n",
      "epoch = 19, cost = 0.3025507628917694, 一  9月 11 14:00:29 CST 2017\n",
      "epoch = 20, cost = 0.3004065454006195, 一  9月 11 14:00:51 CST 2017\n",
      "epoch = 21, cost = 0.31214696168899536, 一  9月 11 14:01:11 CST 2017\n",
      "epoch = 22, cost = 0.2916821241378784, 一  9月 11 14:01:37 CST 2017\n",
      "epoch = 23, cost = 0.29284968972206116, 一  9月 11 14:01:57 CST 2017\n",
      "epoch = 24, cost = 0.2827128767967224, 一  9月 11 14:02:17 CST 2017\n",
      "epoch = 25, cost = 0.2746976613998413, 一  9月 11 14:02:39 CST 2017\n",
      "epoch = 26, cost = 0.2746012210845947, 一  9月 11 14:03:00 CST 2017\n",
      "epoch = 27, cost = 0.26830434799194336, 一  9月 11 14:03:21 CST 2017\n",
      "epoch = 28, cost = 0.26983147859573364, 一  9月 11 14:03:46 CST 2017\n",
      "epoch = 29, cost = 0.26192811131477356, 一  9月 11 14:04:17 CST 2017\n",
      "epoch = 30, cost = 0.2660239338874817, 一  9月 11 14:04:45 CST 2017\n",
      "epoch = 31, cost = 0.26564839482307434, 一  9月 11 14:05:12 CST 2017\n",
      "epoch = 32, cost = 0.25634315609931946, 一  9月 11 14:05:38 CST 2017\n",
      "epoch = 33, cost = 0.25527435541152954, 一  9月 11 14:06:03 CST 2017\n",
      "epoch = 34, cost = 0.256669282913208, 一  9月 11 14:06:32 CST 2017\n",
      "epoch = 35, cost = 0.24809548258781433, 一  9月 11 14:07:01 CST 2017\n",
      "epoch = 36, cost = 0.25750964879989624, 一  9月 11 14:07:26 CST 2017\n",
      "epoch = 37, cost = 0.23703020811080933, 一  9月 11 14:07:56 CST 2017\n",
      "epoch = 38, cost = 0.2349323332309723, 一  9月 11 14:08:20 CST 2017\n",
      "epoch = 39, cost = 0.24557051062583923, 一  9月 11 14:09:03 CST 2017\n",
      "epoch = 40, cost = 0.23313888907432556, 一  9月 11 14:09:35 CST 2017\n",
      "epoch = 41, cost = 0.2354453057050705, 一  9月 11 14:09:59 CST 2017\n",
      "epoch = 42, cost = 0.23493337631225586, 一  9月 11 14:10:22 CST 2017\n",
      "epoch = 43, cost = 0.22841119766235352, 一  9月 11 14:10:43 CST 2017\n",
      "epoch = 44, cost = 0.23436109721660614, 一  9月 11 14:11:06 CST 2017\n",
      "epoch = 45, cost = 0.22151266038417816, 一  9月 11 14:11:27 CST 2017\n",
      "epoch = 46, cost = 0.22303174436092377, 一  9月 11 14:11:48 CST 2017\n",
      "epoch = 47, cost = 0.21544738113880157, 一  9月 11 14:12:08 CST 2017\n",
      "epoch = 48, cost = 0.21680475771427155, 一  9月 11 14:12:29 CST 2017\n",
      "epoch = 49, cost = 0.2142726331949234, 一  9月 11 14:12:50 CST 2017\n",
      "epoch = 50, cost = 0.2162090688943863, 一  9月 11 14:13:10 CST 2017\n",
      "epoch = 51, cost = 0.21874532103538513, 一  9月 11 14:13:30 CST 2017\n",
      "epoch = 52, cost = 0.2111402153968811, 一  9月 11 14:13:51 CST 2017\n",
      "epoch = 53, cost = 0.21109133958816528, 一  9月 11 14:14:20 CST 2017\n",
      "epoch = 54, cost = 0.21135161817073822, 一  9月 11 14:14:41 CST 2017\n",
      "epoch = 55, cost = 0.24234363436698914, 一  9月 11 14:15:02 CST 2017\n",
      "epoch = 56, cost = 0.20665229856967926, 一  9月 11 14:15:23 CST 2017\n",
      "epoch = 57, cost = 0.20522940158843994, 一  9月 11 14:15:43 CST 2017\n",
      "epoch = 58, cost = 0.20673266053199768, 一  9月 11 14:16:04 CST 2017\n",
      "epoch = 59, cost = 0.2033841609954834, 一  9月 11 14:16:25 CST 2017\n",
      "epoch = 60, cost = 0.2040555775165558, 一  9月 11 14:16:46 CST 2017\n",
      "epoch = 61, cost = 0.19967299699783325, 一  9月 11 14:17:07 CST 2017\n",
      "epoch = 62, cost = 0.19989632070064545, 一  9月 11 14:17:29 CST 2017\n",
      "epoch = 63, cost = 0.19647648930549622, 一  9月 11 14:17:50 CST 2017\n",
      "epoch = 64, cost = 0.1960475742816925, 一  9月 11 14:18:11 CST 2017\n",
      "epoch = 65, cost = 0.19815397262573242, 一  9月 11 14:18:32 CST 2017\n",
      "epoch = 66, cost = 0.19785182178020477, 一  9月 11 14:18:54 CST 2017\n",
      "epoch = 67, cost = 0.20672659575939178, 一  9月 11 14:19:27 CST 2017\n",
      "epoch = 68, cost = 0.19292138516902924, 一  9月 11 14:19:50 CST 2017\n",
      "epoch = 69, cost = 0.1969781070947647, 一  9月 11 14:20:12 CST 2017\n",
      "epoch = 70, cost = 0.1858220398426056, 一  9月 11 14:20:35 CST 2017\n",
      "epoch = 71, cost = 0.18818806111812592, 一  9月 11 14:20:59 CST 2017\n",
      "epoch = 72, cost = 0.18416015803813934, 一  9月 11 14:21:38 CST 2017\n",
      "epoch = 73, cost = 0.18353457748889923, 一  9月 11 14:22:06 CST 2017\n",
      "epoch = 74, cost = 0.183407261967659, 一  9月 11 14:22:28 CST 2017\n",
      "epoch = 75, cost = 0.18738940358161926, 一  9月 11 14:22:49 CST 2017\n",
      "epoch = 76, cost = 0.18180537223815918, 一  9月 11 14:23:11 CST 2017\n",
      "epoch = 77, cost = 0.18417637050151825, 一  9月 11 14:23:32 CST 2017\n",
      "epoch = 78, cost = 0.17878825962543488, 一  9月 11 14:23:54 CST 2017\n",
      "epoch = 79, cost = 0.18798017501831055, 一  9月 11 14:24:15 CST 2017\n",
      "epoch = 80, cost = 0.17875893414020538, 一  9月 11 14:24:36 CST 2017\n",
      "epoch = 81, cost = 0.18011018633842468, 一  9月 11 14:24:58 CST 2017\n",
      "epoch = 82, cost = 0.1841978281736374, 一  9月 11 14:25:19 CST 2017\n",
      "epoch = 83, cost = 0.1741950362920761, 一  9月 11 14:25:43 CST 2017\n",
      "epoch = 84, cost = 0.1735062152147293, 一  9月 11 14:26:04 CST 2017\n",
      "epoch = 85, cost = 0.17261318862438202, 一  9月 11 14:26:26 CST 2017\n",
      "epoch = 86, cost = 0.16944652795791626, 一  9月 11 14:26:48 CST 2017\n",
      "epoch = 87, cost = 0.16752579808235168, 一  9月 11 14:27:09 CST 2017\n",
      "epoch = 88, cost = 0.17739799618721008, 一  9月 11 14:27:30 CST 2017\n",
      "epoch = 89, cost = 0.16872574388980865, 一  9月 11 14:27:51 CST 2017\n",
      "epoch = 90, cost = 0.16559623181819916, 一  9月 11 14:28:19 CST 2017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9ed015848c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         _ = sess.run([train_step], feed_dict = {\n\u001b[1;32m     16\u001b[0m                                             \u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtraining_img_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                             ys: [df_croped_face_img_training['PSPI_one_hot'].iloc[i]]})\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# 這一輪訓練後的整體誤差\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m--> 971\u001b[0;31m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0m\u001b[1;32m    972\u001b[0m             raise ValueError(\n\u001b[1;32m    973\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     \"\"\"\n\u001b[0;32m--> 710\u001b[0;31m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       if (not isinstance(value, compat.bytes_or_text_types) and\n\u001b[0m\u001b[1;32m     34\u001b[0m           self._value != value):\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ambiguous dimension: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 模型訓練前的整體誤差\n",
    "cost = sess.run(loss, feed_dict = {\n",
    "                                    xs: training_img_dataset,\n",
    "                                    ys: df_croped_face_img_training['PSPI_one_hot'].tolist() })\n",
    "print(\"epoch = None, cost = {}, {}\".format(cost , subprocess.getoutput('date')))\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 訓練\n",
    "for epoch in range(200):\n",
    "    train_squence = [i for i in range(len(training_img_dataset))] #產生線性的資料流水號序列\n",
    "    np.random.shuffle(train_squence)\n",
    "    \n",
    "    # 這一輪訓練\n",
    "    for i in train_squence:\n",
    "        _ = sess.run([train_step], feed_dict = {\n",
    "                                            xs: [training_img_dataset[i]],\n",
    "                                            ys: [df_croped_face_img_training['PSPI_one_hot'].iloc[i]]})\n",
    "    \n",
    "    # 這一輪訓練後的整體誤差\n",
    "    cost = sess.run(loss, feed_dict = {\n",
    "                                    xs: training_img_dataset,\n",
    "                                    ys: df_croped_face_img_training['PSPI_one_hot'].tolist()})\n",
    "    print(\"epoch = {}, cost = {}, {}\".format(epoch, cost , subprocess.getoutput('date')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4Qcy9f8TTESVdWy8sAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f491932f400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion_matrix\n",
    "inference_class = sess.run(tf.arg_max(\n",
    "                                sess.run(layer_output, feed_dict={xs:validation_img_dataset}), dimension=1))\n",
    "# 不吃 one-hot labels，只吃 1D-arry\n",
    "confusion_matrix = sess.run(\n",
    "        tf.confusion_matrix(df_croped_face_img_validation['PSPI'].tolist(),\n",
    "                            inference_class,\n",
    "                            num_classes = len(df_croped_face_img_validation['PSPI'].unique())))\n",
    "from sklearn.preprocessing import normalize\n",
    "plt.imshow(normalize(confusion_matrix, norm='l1'), cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCCs = [[ 1.          0.75451573]\n",
      "        [ 0.75451573  1.        ]]\n",
      "MSE  = 0.7761342073486286\n"
     ]
    }
   ],
   "source": [
    "# 計算實驗結果\n",
    "from sklearn.metrics import mean_squared_error\n",
    "predict_result = inference_class\n",
    "ground_truth = df_croped_face_img_validation['PSPI'].values\n",
    "print(\"PCCs = {}\".format(np.corrcoef(x=ground_truth, y=predict_result)).replace('\\n','\\n'+' '*7))\n",
    "print(\"MSE  = {}\".format(mean_squared_error(ground_truth, predict_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver_path = saver.save(sess, \"trained_model/train_net_by_pain_face_classification.ckpt\")\n",
    "print(saver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
